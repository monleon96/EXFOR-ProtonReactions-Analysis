{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nueronal Network Autoencoder Method for Outlier Detection\n",
    "**Author:** Juan A. Monleón de la Lluvia  \n",
    "**Date:** 29-08-2023  \n",
    "\n",
    "## Description\n",
    "This Jupyter Notebook demonstrates a comprehensive approach for identifying outliers in data sets resulting from proton-induced experiments. Utilizing a neural network autoencoder model, the notebook provides an end-to-end guide, covering data preprocessing, model building, evaluation, and anomaly detection. The focus is to offer a replicable, step-by-step methodology for efficient outlier analysis in scientific datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from EXFOR_ProtonReactions_UtilityFunctions import *\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"sklearn\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'D:\\OneDrive\\ETSII\\MASTER\\TFM\\Scripts\\exfortables\\EXFOR_ProtonReactions_Classified_Group_1.csv'\n",
    "df = pd.read_csv(path)\n",
    "df = clean_dataframe(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the IDs and drop them from the dataframe\n",
    "x4_id = df['X4_ID']\n",
    "df = df.drop(columns=['X4_ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Splitting (Train/Test)\n",
    "X_train, X_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Data Scaling\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Architecture\n",
    "input_dim = X_train.shape[1]\n",
    "encoding_dim = int(input_dim / 2)  # por simplicidad, pero puedes ajustarlo según necesites\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "encoder = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "decoder = Dense(input_dim, activation='sigmoid')(encoder)\n",
    "\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Compilation\n",
    "autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training\n",
    "history = autoencoder.fit(X_train, X_train, epochs=50, batch_size=256, validation_data=(X_test, X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Plot\n",
    "plt.plot(history.history['loss'], label='Loss of training')\n",
    "plt.plot(history.history['val_loss'], label='Loss of validation')\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preformance Metrics\n",
    "predictions = autoencoder.predict(X_test)\n",
    "mse = np.mean(np.power(X_test - predictions, 2), axis=1)\n",
    "print(f'MSE: {np.mean(mse)}')\n",
    "\n",
    "mean_value = np.mean(X_train, axis=0)\n",
    "mse_naive = np.mean((X_test - mean_value) ** 2)\n",
    "print(f\"MSE of naive model: {mse_naive}\")\n",
    "\n",
    "std_value = np.std(X_train, axis=0)\n",
    "print(f\"Standard deviation: {std_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the DataFrame for Outlier Detection\n",
    "df = pd.read_csv(path)\n",
    "df = clean_dataframe(df)\n",
    "X4_ID_column = df['X4_ID'].copy()\n",
    "df_without_X4_ID = df.drop('X4_ID', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling and Predictions\n",
    "df_scaled_complete = scaler.transform(df_without_X4_ID)\n",
    "predictions = autoencoder.predict(df_scaled_complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting Outliers\n",
    "reconstruction_error = np.mean(np.power(df_scaled_complete - predictions, 2), axis=1)\n",
    "threshold = np.percentile(reconstruction_error, 99.0)\n",
    "anomalies_col2 = reconstruction_error > threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-processing\n",
    "df_original_values = pd.DataFrame(scaler.inverse_transform(df_scaled_complete), columns=df_without_X4_ID.columns, index=df.index)\n",
    "df_original_values['X4_ID'] = X4_ID_column\n",
    "df_original_values['Outliers'] = anomalies_col2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the Outliers\n",
    "outliers_df = df_original_values[df_original_values['Outliers'] == True].drop('Outliers', axis=1)\n",
    "print('Porcentaje de outliers: {:.2f}%'.format(len(outliers_df)/len(df)*100))\n",
    "outliers_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual Representation and Verification of Outliers\n",
    "\n",
    "For the visual representations, the whole data set need to be loaded into memory. This is done by using the `read_experiments_from_binary` function, but also could be done by using the `read_experiments_from_txt` function, both available in the `EXFOR_ProtonReactions_UtilityFunctions.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = read_experiments_from_binary('EXFOR_ProtonReactions_Database.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_outliers(outliers_df, experiments, ylog=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFM_ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
